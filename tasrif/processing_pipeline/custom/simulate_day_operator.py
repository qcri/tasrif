"""
Operator to shorten a dataframe that has a date column per row per participant.
"""

import numpy as np
import pandas as pd

from tasrif.processing_pipeline import ProcessingOperator


class SimulateDayOperator(ProcessingOperator):
    """
    Sample activity of a participant given the participant's activiy mean and standard deviation.
    This operator is usually used after AggregateOperator.

    """
    def __init__(  # pylint: disable=R0913
            self,
            sample_by,
            distribution_parameter_columns,
            distribution_type='normal',
            samples=1,
            sample_column_name='sample',
            output_format='wide'):
        """Creates a new instance of SimulateDayOperator

        Args:
            sample_by : str or list of str
                Name of the feature(s) to sample from. An example can be`['participantID', 'date']`
            distribution_parameter_columns : list of str
                list of columns in data_frame that represent the input
                to `distribution_type`. For example, `if distribution_type='normal'`,
                then `distribution_parameter_columns` should provide the mean and standard deviation
                columns as `['mu', 'std']`.
            distribution_type : str
                Defaults to Guassian normal distribution. More distributions
                can be picked from 'here <https://numpy.org/doc/stable/reference/random/generator.html>'_
            samples : int
                Number of samples to generate from the distribution of each datapoint
                in `data_feature_name`. Suppose that date_feature_name had the times representing
                mean and standard deviation of every 15-minute in a day per participant.
                This means that we have 1440 data points per participant since
                there are 1440 15-minutes in a day. If we set samples=3, then
                the operator will generate 3*1440 data points.
            sample_column_name : str
                Name of the sample column generated by this operator
            output_format : str
                'wide' for samples to be in columns. 'long' for samples to be in a single column.


        Example
        -------

        >>> import numpy as np
        >>> import pandas as pd
        >>>
        >>> from tasrif.processing_pipeline.pandas import RenameOperator
        >>> from tasrif.processing_pipeline.custom import SimulateDayOperator, AggregateOperator
        >>> # -
        >>>
        >>> dates = pd.date_range("2016-12-31", "2020-01-03", freq="15T").to_series()
        >>> df = pd.DataFrame()
        >>> df["Date"] = dates
        >>> df["Steps"] = np.random.randint(0, 10, size=len(df))
        >>> df['ID'] = 'user1'
        >>>
        >>> # +
        >>> operator = AggregateOperator(
        ...     groupby_feature_names =["ID", df.index.hour],
        ...     aggregation_definition= {"Steps": ["mean", "std"]}
        ... )
        >>>
        >>> df = operator.process(df)[0]
        >>>
        >>> operator = RenameOperator(
        ...     columns={'level_1': 'hour'}
        ... )
        >>>
        >>> df = operator.process(df)[0]
        >>> # -
        >>>
        >>> # +
        >>> operator = SimulateDayOperator(
        ...     sample_by=['hour', 'ID'],
        ...     distribution_parameter_columns=['Steps_mean', 'Steps_std'],
        ...     distribution_type='normal',
        ...     samples=2,
        ...     sample_column_name="sample",
        ...     output_format='wide'
        ... )
        >>>
        >>> operator.process(df)[0]
        >>>
        hour    ID  sample0     sample1
        0   user1   -2.768625   3.024987
        1   user1   8.183318    7.567774
        2   user1   0.626714    -1.691519
        3   user1   6.105097    1.716404
        4   user1   7.463729    6.388056
        5   user1   9.684482    -3.017150
        6   user1   11.681545   3.092835
        7   user1   5.944470    6.724310
        8   user1   -1.669262   5.597123
        9   user1   6.585855    3.306794
        10  user1   3.259229    4.974119
        11  user1   4.229145    5.718534
        12  user1   4.371528    1.542301
        13  user1   0.718107    5.562225
        14  user1   -0.935345   6.939493
        15  user1   2.505013    5.450241
        16  user1   5.709531    0.174508
        17  user1   8.060160    2.629553
        18  user1   6.031912    6.338992
        19  user1   1.994668    7.356310
        20  user1   7.950334    8.321414
        21  user1   2.345894    5.414733
        22  user1   6.026677    2.623633
        23  user1   7.324242    2.336856

        """
        super().__init__()
        self.sample_by = sample_by
        self.distribution_parameter_columns = distribution_parameter_columns
        self.distribution_type = distribution_type
        self.samples = samples
        self.sample_column_name = sample_column_name
        self.output_format = output_format

    def _process(self, *data_frames):
        """Processes the passed data frame as per the configuration define in the constructor.

        Args:
            *data_frames (list of pd.DataFrame):
              Variable number of pandas dataframes to be processed

        Returns:
            pd.DataFrame -or- list[pd.DataFrame]
                Processed dataframe(s) resulting from applying the operator
        """

        processed = []
        for data_frame in data_frames:
            samples = data_frame.groupby(self.sample_by).apply(self._sample_from_distribution)
            samples = pd.DataFrame(index=samples.index, data=samples.values.tolist())

            if self.output_format == 'wide':
                columns = [self.sample_column_name + str(i) for i in range(self.samples)]
                samples.columns = columns
            else:
                samples = samples.melt(var_name=self.sample_column_name,
                                       value_name=self.sample_column_name + '_value',
                                       value_vars=samples.columns,
                                       ignore_index=False)

            processed.append(samples)

        return processed

    def _sample_from_distribution(self, datapoint):
        """Sample from a distribution given the distribution parameters

        Args:
            datapoint (pd.DataFrame):
              groupby object representing distribution parameters

        Returns:
            samples
                samples received from distribution

        """
        distribution_parameters = datapoint[
            self.distribution_parameter_columns]
        samples = getattr(
            np.random,
            self.distribution_type)(*distribution_parameters.values[0], size=self.samples)
        return samples
